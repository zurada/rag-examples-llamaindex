# brew install poppler
# brew install tesseract
# brew install tesseract-lang
# pip install llama-index-llms-openai rank_bm25 llama-index-retrievers-bm25

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document, StorageContext, load_index_from_storage
from llama_index.core.readers.base import BaseReader
from llama_index.readers.file import DocxReader
from llama_index.core.agent.workflow import ReActAgent
from llama_index.core.workflow import Context
from llama_index.core.tools import FunctionTool
from llama_index.core.callbacks import CallbackManager
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# --- NOWE IMPORTY DLA HYBRID SEARCH ---
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

import asyncio
import pytesseract
from pdf2image import convert_from_path
import os
import logging
import sys
import time
import psutil

# W≈ÇƒÖcz szczeg√≥≈Çowe logowanie
logging.basicConfig(
    stream=sys.stdout,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# --- 1. KONFIGURACJA LOKALNEGO OCR DLA PDF ---
from concurrent.futures import ThreadPoolExecutor

def process_single_page(page_data):
    """Przetwarza jednƒÖ stronƒô PDF (uruchamiane r√≥wnolegle)"""
    page_num, image = page_data
    custom_config = r'--oem 1 --psm 3'
    page_text = pytesseract.image_to_string(image, lang='pol+eng', config=custom_config)
    return page_num, page_text

class LocalOCRReader(BaseReader):
    def load_data(self, file_path, extra_info=None):
        print(f"üîÑ OCR PDF: {os.path.basename(file_path)}...")
        text = ""
        try:
            # DPI=150, 8 wƒÖtk√≥w dla M4 Pro
            images = convert_from_path(file_path, dpi=150, thread_count=8)
            print(f"   üìÑ Stron do OCR: {len(images)}")

            # 8 worker√≥w dla OCR
            with ThreadPoolExecutor(max_workers=8) as executor:
                page_data = list(enumerate(images, 1))
                results = list(executor.map(process_single_page, page_data))

            results.sort(key=lambda x: x[0])
            for page_num, page_text in results:
                text += f"\n--- Strona {page_num} ---\n{page_text}"

            print(f"‚úÖ Zako≈Ñczono OCR: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd OCR: {e}")
            return []
        return [Document(text=text, extra_info=extra_info or {})]

# --- 2. DEFINICJA OBS≈ÅUGI PLIK√ìW ---
file_extractor = {
    ".pdf": LocalOCRReader(),
    ".docx": DocxReader()
}

# --- 3. USTAWIENIA LLM (OpenAI) ---
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

# OpenAI LLM - MUSISZ USTAWIƒÜ KLUCZ API:
# export OPENAI_API_KEY="sk-..."
Settings.llm = OpenAI(
    model="gpt-5.2",  # Zmieniono na gpt-4o (gpt-5.2 nie istnieje)
    temperature=0.1,
    max_tokens=4000,
)

print(f"ü§ñ U≈ºywam modelu: {Settings.llm.model}")

# --- 4. ≈ÅADOWANIE LUB TWORZENIE INDEKSU ---
PERSIST_DIR = "./storage"

if os.path.exists(PERSIST_DIR):
    print("üíæ ≈Åadowanie zapisanego indeksu...")
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    index = load_index_from_storage(storage_context)
    print("‚úÖ Za≈Çadowano indeks z dysku!")
else:
    print("üìÇ Skanowanie katalogu 'data' i podfolder√≥w...")

    # Sprawd≈∫ liczbƒô rdzeni CPU
    import multiprocessing
    cpu_count = multiprocessing.cpu_count()
    print(f"üíª Wykryto {cpu_count} rdzeni CPU")

    reader = SimpleDirectoryReader(
        "data",
        file_extractor=file_extractor,
        recursive=True
    )

    # R√≥wnoleg≈Çe przetwarzanie plik√≥w
    from threading import Lock
    documents = []
    files = reader.input_files
    processed_count = [0]
    lock = Lock()

    def process_file(file_path):
        """Przetwarza pojedynczy plik"""
        print(f"‚ñ∂Ô∏è  Start: {os.path.basename(file_path)}")
        file_reader = SimpleDirectoryReader(
            input_files=[file_path],
            file_extractor=file_extractor
        )
        docs = file_reader.load_data()
        with lock:
            processed_count[0] += 1
            pct = (processed_count[0] / len(files)) * 100
            print(f"‚úÖ [{processed_count[0]}/{len(files)} - {pct:.1f}%] Zako≈Ñczono: {os.path.basename(file_path)}")
            return docs

    print("‚ö° Przetwarzam pliki r√≥wnolegle...\n")
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = executor.map(process_file, files)
        for docs in results:
            documents.extend(docs)

    print(f"\nüìö Za≈Çadowano ≈ÇƒÖcznie {len(documents)} fragment√≥w dokument√≥w.")

    index = VectorStoreIndex.from_documents(documents)
    index.storage_context.persist(persist_dir=PERSIST_DIR)
    print(f"üíæ Zapisano indeks do {PERSIST_DIR}")

# --- 5. KONFIGURACJA HYBRID SEARCH (ZMIANA G≈Å√ìWNA) ---
print("‚öôÔ∏è Konfiguracja Hybrid Search (Vector + BM25)...")

# 1. Retriever Wektorowy (Semantyczny - rozumie znaczenie)
vector_retriever = index.as_retriever(similarity_top_k=20)

# 2. Retriever S≈Ç√≥w Kluczowych (BM25 - precyzyjny dla nazw w≈Çasnych i liczb)
bm25_retriever = BM25Retriever.from_defaults(
    docstore=index.docstore, 
    similarity_top_k=20
)

# 3. Po≈ÇƒÖczenie (Fusion) - Reciprocal Rank Fusion
retriever = QueryFusionRetriever(
    [vector_retriever, bm25_retriever],
    similarity_top_k=25,  # Zwr√≥ƒá top 15 najlepszych fragment√≥w z obu metod
    num_queries=1,        # Nie generuj dodatkowych pyta≈Ñ (oszczƒôdno≈õƒá czasu/token√≥w)
    mode="reciprocal_rerank",  # Poprawiona nazwa trybu
)

# 4. Budowa silnika zapyta≈Ñ
query_engine = RetrieverQueryEngine.from_args(
    retriever=retriever,
    llm=Settings.llm,
    response_mode="tree_summarize" # Tryb, kt√≥ry lepiej sk≈Çada informacje z wielu kawa≈Çk√≥w
)

# --- 6. AGENT I NARZƒòDZIA ---
def multiply(a: float, b: float) -> float:
    """Mno≈ºy dwie liczby."""
    return a * b

def search_documents(query: str) -> str:
    """
    Wyszukuje informacje w dokumentach. U≈ºywa wyszukiwania hybrydowego (s≈Çowa kluczowe + kontekst).
    U≈ºyj tego narzƒôdzia gdy u≈ºytkownik pyta o zawarto≈õƒá plik√≥w, PDF√≥w, SWZ lub Worda.
    """
    response = query_engine.query(query)
    return str(response)

# Konwertuj funkcje na FunctionTool
search_tool = FunctionTool.from_defaults(fn=search_documents)

# Callback handler do monitorowania
from llama_index.core.callbacks import CBEventType, EventPayload
from llama_index.core.callbacks.base_handler import BaseCallbackHandler

class VerboseCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        super().__init__(event_starts_to_ignore=[], event_ends_to_ignore=[])
        self.llm_call_count = 0
        self.start_time = time.time()
        self.token_count = 0

    def on_event_start(self, event_type, payload=None, event_id=None, **kwargs):
        if event_type == CBEventType.LLM:
            self.llm_call_count += 1
            elapsed = time.time() - self.start_time
            print(f"\nü§ñ [Wywo≈Çanie OpenAI #{self.llm_call_count}] (czas: {elapsed:.1f}s)")
            if payload and EventPayload.MESSAGES in payload:
                messages = payload[EventPayload.MESSAGES]
                if messages:
                    last_msg = str(messages[-1])[:200]
                    print(f"   üí¨ {last_msg}...")

    def on_event_end(self, event_type, payload=None, event_id=None, **kwargs):
        if event_type == CBEventType.LLM:
            print(f"   ‚úÖ Odpowied≈∫ otrzymana")
            if payload and hasattr(payload.get(EventPayload.RESPONSE, None), 'raw'):
                raw = payload[EventPayload.RESPONSE].raw
                if hasattr(raw, 'usage') and raw.usage is not None:
                    usage = raw.usage
                    print(f"   üìä Tokeny: {usage.prompt_tokens} prompt + {usage.completion_tokens} completion = {usage.total_tokens} total")
                    self.token_count += usage.total_tokens

    def start_trace(self, trace_id=None):
        pass

    def end_trace(self, trace_id=None, trace_map=None):
        pass

verbose_handler = VerboseCallbackHandler()
callback_manager = CallbackManager([verbose_handler])
Settings.callback_manager = callback_manager

# Utw√≥rz agenta
agent = ReActAgent(
    tools=[search_tool],
    llm=Settings.llm,
)

async def main():
    ctx = Context(agent)

    print("\n" + "="*60)
    print("PYTANIE:")
    print("="*60)
    question = """
Jeste≈õ Bezwzglƒôdnym Audytorem Dokumentacji Przetargowej.
Twoim celem jest stworzenie "Checklisty Twardych Wymaga≈Ñ" (Must-Have) na podstawie analizy dokument√≥w (SWZ, OPZ, Wz√≥r Umowy).

ZADANIE DLA AGENTA:
1. Przeszukaj dokumenty u≈ºywajƒÖc narzƒôdzia `search_documents`.
   SZUKAJ S≈Å√ìW KLUCZOWYCH SUGERUJƒÑCYCH WYM√ìG:
   - "wym√≥g", "wymaga siƒô", "wymagane"
   - "musi", "nale≈ºy", "wykonawca jest zobowiƒÖzany"
   - "kryterium dopuszczajƒÖce", "warunek graniczny"
   - oraz standardowe: "wadium", "kary", "do≈õwiadczenie", "ubezpieczenie", "gwarancja".

2. Ignoruj "lanie wody". Szukaj konkret√≥w: kwot, dat, procent√≥w, liczby os√≥b, lat do≈õwiadczenia.

3. SFORMATUJ WYNIK JAKO JSON (DYNAMICZNA STRUKTURA):
   - Nie u≈ºywaj sztywnych nazw kategorii ani numeracji (np. "1_terminy").
   - Stw√≥rz kategorie na podstawie tego, co faktycznie znajdziesz w dokumencie (np. "UBEZPIECZENIE_OC", "KARY_UMOWNE", "KIEROWNIK_BUDOWY").
   - Je≈õli dokument milczy na dany temat, NIE tw√≥rz pustej kategorii.

FORMAT WYJ≈öCIOWY (JSON):
{
  "meta_info": {
    "znalezione_dokumenty": ["lista plik√≥w"],
    "nazwa_postepowania": "..."
  },
  "WYKRYTE_WYMAGANIA": {
    "NAZWA_KATEGORII_WIELKIMI_LITERAMI (np. WADIUM)": [
      {
        "wymog": "Kr√≥tki opis czego dotyczy (np. 'Kwota wadium')",
        "szczegoly_wartosc": "Konkretna warto≈õƒá (np. '50 000 PLN', '5 lat do≈õwiadczenia', 'Gwarancja bankowa')",
        "status": "WYMAGANE / OPCJONALNE / BRAK DANYCH",
        "zrodlo": "Nazwa pliku i przybli≈ºona lokalizacja (np. Rozdzia≈Ç 4, pkt 2)"
      },
      {
        "wymog": "Forma wniesienia",
        "szczegoly_wartosc": "PieniƒÖdz, gwarancja bankowa lub ubezpieczeniowa",
        "status": "WYMAGANE",
        "zrodlo": "..."
      }
    ],
    "NAZWA_INNEJ_ZNALEZIONEJ_KATEGORII (np. KARY_UMOWNE)": [
      {
        "wymog": "Limit kar umownych",
        "szczegoly_wartosc": "20% warto≈õci umowy brutto",
        "status": "WYMAGANE",
        "zrodlo": "Wz√≥r Umowy ¬ß15"
      }
    ]
  },
  "UWAGI_KRYTYCZNE": [
    "Tutaj wpisz ostrze≈ºenia, je≈õli brakuje kluczowych element√≥w (np. brak informacji o terminie realizacji mimo znalezienia SWZ)."
  ]
}
"""
    print(question)
    print("="*60)

    # Monitor wydajno≈õci
    process = psutil.Process()
    ram_before = process.memory_info().rss / 1024 / 1024
    start_time = time.time()
    verbose_handler.start_time = start_time

    print(f"\nüìä RAM przed: {ram_before:.1f} MB")
    print("‚è≥ Rozpoczynam zapytanie do OpenAI...\n")

    result = await agent.run(ctx=ctx, user_msg=question)

    end_time = time.time()
    ram_after = process.memory_info().rss / 1024 / 1024
    duration = end_time - start_time

    print("\n" + "="*60)
    print("ODPOWIED≈π AGENTA:")
    print("="*60)
    print(result)
    print("="*60)

    print("\n" + "="*60)
    print("üìä METRYKI WYDAJNO≈öCI:")
    print(f"‚è±Ô∏è  Czas: {duration:.1f}s")
    print(f"üî¢ Wywo≈Çania OpenAI: {verbose_handler.llm_call_count}")
    print(f"üéØ Tokeny: {verbose_handler.token_count}")
    estimated_cost = (verbose_handler.token_count / 1_000_000) * 2.50 # Przybli≈ºony koszt mieszany gpt-4o
    print(f"üíµ Szacunkowy koszt: ~${estimated_cost:.4f}")
    print("="*60)

if __name__ == "__main__":
    asyncio.run(main())