# brew install poppler
# brew install tesseract
# brew install tesseract-lang

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document, StorageContext, load_index_from_storage
from llama_index.core.readers.base import BaseReader
from llama_index.readers.file import DocxReader  # <--- IMPORT DLA WORD
from llama_index.core.agent.workflow import ReActAgent
from llama_index.core.workflow import Context
from llama_index.core.tools import FunctionTool
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import asyncio
import pytesseract
from pdf2image import convert_from_path
import os
import logging
import sys
import time
import psutil

# WÅ‚Ä…cz szczegÃ³Å‚owe logowanie
logging.basicConfig(
    stream=sys.stdout,
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# WÅ‚Ä…cz logi dla LlamaIndex workflow
logging.getLogger("llama_index.core.agent").setLevel(logging.DEBUG)
logging.getLogger("llama_index.core.workflow").setLevel(logging.DEBUG)

# --- 1. KONFIGURACJA LOKALNEGO OCR DLA PDF (Twoja wersja na Mac M4) ---
from concurrent.futures import ThreadPoolExecutor  # Zmiana z ProcessPoolExecutor na ThreadPoolExecutor

def process_single_page(page_data):
    """Przetwarza jednÄ… stronÄ™ PDF (uruchamiane rÃ³wnolegle)"""
    page_num, image = page_data
    # Szybszy config tesseracta: --psm 3 (auto), --oem 1 (LSTM)
    custom_config = r'--oem 1 --psm 3'
    page_text = pytesseract.image_to_string(image, lang='pol+eng', config=custom_config)
    return page_num, page_text

class LocalOCRReader(BaseReader):
    def load_data(self, file_path, extra_info=None):
        print(f"ðŸ”„ OCR PDF: {os.path.basename(file_path)}...")
        text = ""
        try:
            # DPI=150 zamiast domyÅ›lnych 200 = znacznie szybciej, wciÄ…Å¼ czytelne
            # thread_count=8 dla convert_from_path (M4 Pro ma 14-16 rdzeni)
            images = convert_from_path(file_path, dpi=150, thread_count=8)

            print(f"   ðŸ“„ Stron do OCR: {len(images)}")

            # M4 Pro ma 14-16 rdzeni - uÅ¼ywamy 8 workerÃ³w dla OCR
            # ThreadPoolExecutor dziaÅ‚a Å›wietnie bo Tesseract zwalnia GIL
            with ThreadPoolExecutor(max_workers=8) as executor:
                page_data = list(enumerate(images, 1))
                results = list(executor.map(process_single_page, page_data))

            # Sortuj po numerze strony i zÅ‚Ä…cz tekst
            results.sort(key=lambda x: x[0])
            for page_num, page_text in results:
                text += f"\n--- Strona {page_num} ---\n{page_text}"

            print(f"âœ… ZakoÅ„czono OCR: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d OCR: {e}")
            return []
        return [Document(text=text, extra_info=extra_info or {})]

# --- 2. DEFINICJA OBSÅUGI PLIKÃ“W ---

# Tworzymy mapÄ™: rozszerzenie -> odpowiedni czytnik
file_extractor = {
    ".pdf": LocalOCRReader(),  # Nasz wÅ‚asny OCR dla PDF
    ".docx": DocxReader()      # Wbudowany czytnik Worda (wymaga pip install docx2txt)
}

# --- 3. USTAWIENIA LLM (Ollama) ---
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-base-en-v1.5")

# Klasa Ollama ze streamingiem tokenÃ³w
from llama_index.core.llms import ChatResponse, CompletionResponse
from llama_index.core.base.llms.types import ChatMessage, MessageRole

class StreamingOllama(Ollama):
    """Ollama z wydrukowaniem kaÅ¼dego tokena w czasie rzeczywistym"""

    def chat(self, messages, **kwargs):
        """Override chat Å¼eby drukowaÄ‡ tokeny"""
        full_response = ""

        # UÅ¼yj stream_chat do otrzymywania tokenÃ³w
        for chunk in self.stream_chat(messages, **kwargs):
            token = chunk.delta
            if token:
                print(token, end="", flush=True)
                full_response += token

        return ChatResponse(
            message=ChatMessage(role=MessageRole.ASSISTANT, content=full_response),
            raw={"content": full_response}
        )

    def complete(self, prompt, **kwargs):
        """Override complete Å¼eby drukowaÄ‡ tokeny"""
        full_response = ""

        for chunk in self.stream_complete(prompt, **kwargs):
            token = chunk.delta
            if token:
                print(token, end="", flush=True)
                full_response += token

        return CompletionResponse(text=full_response, raw={"content": full_response})

# UtwÃ³rz streaming LLM
Settings.llm = StreamingOllama(
    #model="SpeakLeash/bielik-11b-v3.0-instruct:bf16",
    model="qwen3:30b",
    request_timeout=360.0,
    context_window=8000,
)

# --- 4. ÅADOWANIE LUB TWORZENIE INDEKSU ---
PERSIST_DIR = "./storage"

if os.path.exists(PERSIST_DIR):
    print("ðŸ’¾ Åadowanie zapisanego indeksu...")
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    index = load_index_from_storage(storage_context)
    print("âœ… ZaÅ‚adowano indeks z dysku (bez OCR)!")
else:
    print("ðŸ“‚ Skanowanie katalogu 'data' i podfolderÃ³w...")

    # SprawdÅº liczbÄ™ rdzeni CPU
    import multiprocessing
    cpu_count = multiprocessing.cpu_count()
    print(f"ðŸ’» Wykryto {cpu_count} rdzeni CPU")

    # Najpierw policz wszystkie pliki
    import glob
    pdf_files = glob.glob("data/**/*.pdf", recursive=True)
    docx_files = glob.glob("data/**/*.docx", recursive=True)
    total_files = len(pdf_files) + len(docx_files)

    print(f"ðŸ” Znaleziono {total_files} plikÃ³w ({len(pdf_files)} PDF, {len(docx_files)} DOCX)")
    print("ðŸ“‚ Rozpoczynam Å‚adowanie rÃ³wnolegÅ‚e...\n")

    # Åaduj z progress tracking
    reader = SimpleDirectoryReader(
        "data",
        file_extractor=file_extractor,
        recursive=True
    )

    # RÃ³wnolegÅ‚e przetwarzanie plikÃ³w (M4 Pro ma duÅ¼o rdzeni)
    from threading import Lock
    documents = []
    files = reader.input_files
    processed_count = [0]  # Licznik w liÅ›cie Å¼eby mÃ³c modyfikowaÄ‡ w threadach
    lock = Lock()

    def process_file(file_path):
        """Przetwarza pojedynczy plik"""
        print(f"â–¶ï¸  Start: {os.path.basename(file_path)}")

        # Åaduj pojedynczy plik
        file_reader = SimpleDirectoryReader(
            input_files=[file_path],
            file_extractor=file_extractor
        )
        docs = file_reader.load_data()

        # Thread-safe update licznika i listy
        with lock:
            processed_count[0] += 1
            pct = (processed_count[0] / len(files)) * 100
            print(f"âœ… [{processed_count[0]}/{len(files)} - {pct:.1f}%] ZakoÅ„czono: {os.path.basename(file_path)}")
            return docs

    # Przetwarzaj 4 pliki rÃ³wnoczeÅ›nie (zostaw rdzenie dla OCR wewnÄ…trz kaÅ¼dego pliku)
    print("âš¡ Przetwarzam pliki rÃ³wnolegle...\n")
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = executor.map(process_file, files)
        for docs in results:
            documents.extend(docs)

    print(f"\nðŸ“š ZaÅ‚adowano Å‚Ä…cznie {len(documents)} fragmentÃ³w dokumentÃ³w.")

    index = VectorStoreIndex.from_documents(documents)

    # Zapisz indeks do dysku
    index.storage_context.persist(persist_dir=PERSIST_DIR)
    print(f"ðŸ’¾ Zapisano indeks do {PERSIST_DIR}")

query_engine = index.as_query_engine(
    similarity_top_k=30, 
    response_mode="tree_summarize" # Tryb, ktÃ³ry lepiej skÅ‚ada informacje z wielu kawaÅ‚kÃ³w
)
# --- 5. AGENT ---
def multiply(a: float, b: float) -> float:
    """MnoÅ¼y dwie liczby."""
    return a * b

def search_documents(query: str) -> str:
    """
    Wyszukuje informacje w zaÅ‚adowanych dokumentach (PDF i DOCX).
    UÅ¼yj tego narzÄ™dzia gdy uÅ¼ytkownik pyta o zawartoÅ›Ä‡ plikÃ³w, dokumentÃ³w, PDFÃ³w lub Worda.

    Args:
        query: Zapytanie o informacje z dokumentÃ³w

    Returns:
        OdpowiedÅº zawierajÄ…ca informacje znalezione w dokumentach
    """
    response = query_engine.query(query)
    return str(response)

# Custom callback handler do monitorowania LLM
from llama_index.core.callbacks import CBEventType, EventPayload
from llama_index.core.callbacks.base_handler import BaseCallbackHandler

class VerboseCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        super().__init__(event_starts_to_ignore=[], event_ends_to_ignore=[])
        self.llm_call_count = 0
        self.start_time = time.time()
        self.current_response = ""

    def on_event_start(self, event_type, payload=None, event_id=None, **kwargs):
        if event_type == CBEventType.LLM:
            self.llm_call_count += 1
            elapsed = time.time() - self.start_time
            print(f"\nðŸ¤– [WywoÅ‚anie LLM #{self.llm_call_count}] (czas: {elapsed:.1f}s)")
            if payload and EventPayload.MESSAGES in payload:
                messages = payload[EventPayload.MESSAGES]
                if messages:
                    last_msg = str(messages[-1])[:200]
                    print(f"   ðŸ’¬ {last_msg}...")
            print("   ðŸ”„ OdpowiedÅº: ", end="", flush=True)
            self.current_response = ""

    def on_event_end(self, event_type, payload=None, event_id=None, **kwargs):
        if event_type == CBEventType.LLM:
            if self.current_response == "" and payload and EventPayload.RESPONSE in payload:
                # JeÅ›li nie byÅ‚o streamingu, wydrukuj caÅ‚Ä… odpowiedÅº
                response = str(payload[EventPayload.RESPONSE])
                print(response)
            print(f"\n   âœ… OdpowiedÅº zakoÅ„czona")

    def start_trace(self, trace_id=None):
        pass

    def end_trace(self, trace_id=None, trace_map=None):
        pass

# Konwertuj funkcje na FunctionTool
multiply_tool = FunctionTool.from_defaults(fn=multiply)
search_tool = FunctionTool.from_defaults(fn=search_documents)

# UtwÃ³rz callback handler
verbose_handler = VerboseCallbackHandler()
callback_manager = CallbackManager([verbose_handler])

# Dodaj callback manager do Settings globalnie
Settings.callback_manager = callback_manager

# UtwÃ³rz agenta zgodnie z nowÄ… dokumentacjÄ…
agent = ReActAgent(
    tools=[search_tool],
    llm=Settings.llm,
)

async def main():
    # UtwÃ³rz kontekst dla sesji
    ctx = Context(agent)

    print("\n" + "="*60)
    print("PYTANIE:")
    print("="*60)
    question = question = """
JesteÅ› BezwzglÄ™dnym Audytorem Dokumentacji Przetargowej.
Twoim zadaniem jest znalezienie i wylistowanie twardych wymagaÅ„ (Must-Have), nawet jeÅ›li sÄ… ukryte gÅ‚Ä™boko w dokumentacji.

ZADANIE DLA AGENTA (Krok po kroku):
1. Twoim priorytetem jest znalezienie gÅ‚Ã³wnego dokumentu SWZ (Specyfikacja WarunkÃ³w ZamÃ³wienia) lub SIWZ, PFU (Program Funkcjonalno-UÅ¼ytkowy) oraz OPZ (Opis Przedmiotu ZamÃ³wienia).
2. UÅ¼yj narzÄ™dzia `search_documents` wielokrotnie, wpisujÄ…c precyzyjne frazy kluczowe typowe dla polskich przetargÃ³w.
   
   Sugerowane zapytania do wyszukiwarki (wykonaj je):
   - "RozdziaÅ‚ Warunki UdziaÅ‚u w PostÄ™powaniu wyksztaÅ‚cenie doÅ›wiadczenie"
   - "Wymagane wadium i zabezpieczenie naleÅ¼ytego wykonania"
   - "Kary umowne i terminy realizacji zamÃ³wienia"
   - "Wymagany potencjaÅ‚ kadrowy i osoby skierowane do realizacji"
   - "Åšrodki finansowe lub zdolnoÅ›Ä‡ kredytowa wykonawcy"

3. Ignoruj aneksy Å›rodowiskowe, decyzje administracyjne i ogÃ³lne warunki, chyba Å¼e zawierajÄ… konkretne liczby/wymogi.

FORMAT WYJÅšCIOWY (JSON):
ZwrÃ³Ä‡ wynik jako JSON. JeÅ›li nie znajdziesz informacji dla danej kategorii, wpisz "BRAK DANYCH W POBRANYCH FRAGMENTACH".

{
  "critical_requirements": [
    {
      "category": "KADRA / FINANSE / DOÅšWIADCZENIE / FORMALNE",
      "source_context": "Z jakiego dokumentu/rozdziaÅ‚u to pochodzi?",
      "requirement_raw": "Cytat z dokumentu",
      "value_to_check": "Konkretna wartoÅ›Ä‡ (np. 'Polisa OC 5 mln PLN', 'Kierownik z uprawnieniami mostowymi')"
    }
  ]
}
"""
    print(question)
    print("="*60)

    # Monitor wydajnoÅ›ci
    process = psutil.Process()
    ram_before = process.memory_info().rss / 1024 / 1024  # MB
    start_time = time.time()
    verbose_handler.start_time = start_time  # Reset czasu w handlerze

    print(f"\nðŸ“Š RAM przed: {ram_before:.1f} MB")
    print("â³ Rozpoczynam zapytanie...\n")

    # UÅ¼yj run z kontekstem
    result = await agent.run(ctx=ctx, user_msg=question)

    # Statystyki
    end_time = time.time()
    ram_after = process.memory_info().rss / 1024 / 1024  # MB
    duration = end_time - start_time

    print("\n" + "="*60)
    print("ODPOWIEDÅ¹ AGENTA:")
    print("="*60)
    print(result)
    print("="*60)

    # PokaÅ¼ metryki
    print("\n" + "="*60)
    print("ðŸ“Š METRYKI WYDAJNOÅšCI:")
    print("="*60)
    print(f"â±ï¸  Czas caÅ‚kowity: {duration:.1f}s ({duration/60:.1f} min)")
    print(f"ðŸ’¾ RAM przed: {ram_before:.1f} MB")
    print(f"ðŸ’¾ RAM po: {ram_after:.1f} MB")
    print(f"ðŸ’¾ RÃ³Å¼nica RAM: +{ram_after - ram_before:.1f} MB")

    # UÅ¼yj licznika z callback handlera
    total_llm_calls = verbose_handler.llm_call_count
    print(f"ðŸ”¢ Liczba wywoÅ‚aÅ„ LLM: {total_llm_calls}")
    if total_llm_calls > 0:
        avg_time_per_call = duration / total_llm_calls
        print(f"âš¡ Åšredni czas na wywoÅ‚anie: {avg_time_per_call:.1f}s")
        # Szacujemy ~20-30 tokens/s dla Bielik na M4
        estimated_tokens = int(duration * 25)  # przybliÅ¼ona wartoÅ›Ä‡
        print(f"ðŸŽ¯ Szacowane tokeny wygenerowane: ~{estimated_tokens}")
        print(f"ðŸš€ Szacowana prÄ™dkoÅ›Ä‡: ~{estimated_tokens/duration:.1f} tokens/s")

    print("="*60)

if __name__ == "__main__":
    asyncio.run(main())